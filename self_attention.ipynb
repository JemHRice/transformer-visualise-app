{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04d8bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de258128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax along last dimension\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Single attention head (replaces self_attention)\n",
    "\n",
    "    Args:\n",
    "        Q: queries (seq_len, d_k)\n",
    "        K: keys (seq_len, d_k)\n",
    "        V: values (seq_len, d_v)\n",
    "        mask: optional mask (seq_len, seq_len)\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len, d_v)\n",
    "        attention_weights: (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = (Q @ K.T) / np.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores + mask\n",
    "\n",
    "    attention_weights = softmax(scores)\n",
    "    output = attention_weights @ V\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03476ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, d_model, num_heads):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism\n",
    "\n",
    "    Args:\n",
    "        X: input sequence (seq_len, d_model)\n",
    "        d_model: total dimension\n",
    "        num_heads: number of attention heads\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len, d_model)\n",
    "        all_attention_weights: list of (seq_len, seq_len) for each head\n",
    "    \"\"\"\n",
    "    seq_len, _ = X.shape\n",
    "\n",
    "    # Ensure d_model is divisible by num_heads\n",
    "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "    d_k = d_model // num_heads  # Dimension per head\n",
    "\n",
    "    # Initialize weight matrices for all heads\n",
    "    # In practice, these would be learned parameters\n",
    "    W_q = np.random.randn(d_model, d_model) * 0.01\n",
    "    W_k = np.random.randn(d_model, d_model) * 0.01\n",
    "    W_v = np.random.randn(d_model, d_model) * 0.01\n",
    "    W_o = np.random.randn(d_model, d_model) * 0.01  # Output projection\n",
    "\n",
    "    # Compute Q, K, V for ALL heads at once\n",
    "    Q = X @ W_q  # (seq_len, d_model)\n",
    "    K = X @ W_k  # (seq_len, d_model)\n",
    "    V = X @ W_v  # (seq_len, d_model)\n",
    "\n",
    "    # Reshape to separate heads\n",
    "    # (seq_len, d_model) -> (seq_len, num_heads, d_k)\n",
    "    Q = Q.reshape(seq_len, num_heads, d_k)\n",
    "    K = K.reshape(seq_len, num_heads, d_k)\n",
    "    V = V.reshape(seq_len, num_heads, d_k)\n",
    "\n",
    "    # Transpose to (num_heads, seq_len, d_k) for easier processing\n",
    "    Q = Q.transpose(1, 0, 2)\n",
    "    K = K.transpose(1, 0, 2)\n",
    "    V = V.transpose(1, 0, 2)\n",
    "\n",
    "    # Apply attention to each head\n",
    "    head_outputs = []\n",
    "    all_attention_weights = []\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        head_output, attn_weights = scaled_dot_product_attention(Q[i], K[i], V[i])\n",
    "        head_outputs.append(head_output)\n",
    "        all_attention_weights.append(attn_weights)\n",
    "\n",
    "    # Concatenate all heads\n",
    "    # Stack: (num_heads, seq_len, d_k) -> (seq_len, num_heads, d_k)\n",
    "    concat_heads = np.stack(head_outputs, axis=1)\n",
    "\n",
    "    # Reshape: (seq_len, num_heads, d_k) -> (seq_len, d_model)\n",
    "    concat_heads = concat_heads.reshape(seq_len, d_model)\n",
    "\n",
    "    # Final linear projection\n",
    "    output = concat_heads @ W_o\n",
    "\n",
    "    return output, all_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate positional encoding\n",
    "\n",
    "    Args:\n",
    "        seq_len: length of sequence\n",
    "        d_model: dimension of embeddings\n",
    "\n",
    "    Returns:\n",
    "        pos_encoding: (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    # Initialize matrix\n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "\n",
    "    # Create position indices [0, 1, 2, ..., seq_len-1]\n",
    "    position = np.arange(seq_len)[:, np.newaxis]  # (seq_len, 1)\n",
    "\n",
    "    # Create dimension indices [0, 2, 4, ..., d_model-2]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "    # Apply sine to even indices\n",
    "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "\n",
    "    # Apply cosine to odd indices\n",
    "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Layer normalization\n",
    "\n",
    "    Args:\n",
    "        x: input (seq_len, d_model)\n",
    "        epsilon: small constant for numerical stability\n",
    "\n",
    "    Returns:\n",
    "        normalized: (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, d_model, d_ff):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network\n",
    "    FFN(x) = ReLU(xW1 + b1)W2 + b2\n",
    "\n",
    "    Args:\n",
    "        x: input (seq_len, d_model)\n",
    "        d_model: model dimension\n",
    "        d_ff: hidden dimension (typically 4 * d_model)\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    # Initialize weights (in practice, these are learned)\n",
    "    W1 = np.random.randn(d_model, d_ff) * 0.01\n",
    "    b1 = np.zeros(d_ff)\n",
    "    W2 = np.random.randn(d_ff, d_model) * 0.01\n",
    "    b2 = np.zeros(d_model)\n",
    "\n",
    "    # First linear layer + ReLU\n",
    "    hidden = np.maximum(0, x @ W1 + b1)  # ReLU activation\n",
    "\n",
    "    # Second linear layer\n",
    "    output = hidden @ W2 + b2\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(x, d_model, num_heads, d_ff):\n",
    "    \"\"\"\n",
    "    Single Transformer encoder layer\n",
    "\n",
    "    Args:\n",
    "        x: input (seq_len, d_model)\n",
    "        d_model: model dimension\n",
    "        num_heads: number of attention heads\n",
    "        d_ff: feed-forward hidden dimension\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    # 1. Multi-head self-attention\n",
    "    attn_output, _ = multi_head_attention(x, d_model, num_heads)\n",
    "\n",
    "    # 2. Add & Norm (residual connection)\n",
    "    x = layer_norm(x + attn_output)\n",
    "\n",
    "    # 3. Feed-forward network\n",
    "    ff_output = feed_forward(x, d_model, d_ff)\n",
    "\n",
    "    # 4. Add & Norm (residual connection)\n",
    "    output = layer_norm(x + ff_output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7650d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(x, num_layers, d_model, num_heads, d_ff):\n",
    "    \"\"\"\n",
    "    Stack of encoder layers\n",
    "\n",
    "    Args:\n",
    "        x: input with positional encoding (seq_len, d_model)\n",
    "        num_layers: number of encoder layers to stack\n",
    "        d_model: model dimension\n",
    "        num_heads: number of attention heads\n",
    "        d_ff: feed-forward hidden dimension\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    # Pass through each encoder layer\n",
    "    for i in range(num_layers):\n",
    "        x = encoder_layer(x, d_model, num_heads, d_ff)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db989e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRANSFORMER ENCODER TEST\n",
      "==================================================\n",
      "\n",
      "1. Single Encoder Layer:\n",
      "   Input shape: (10, 512)\n",
      "   Output shape: (10, 512)\n",
      "   ✓ Shapes match!\n",
      "\n",
      "2. Full Encoder (6 layers):\n",
      "   Input shape: (10, 512)\n",
      "   Output shape: (10, 512)\n",
      "   ✓ Shapes match!\n",
      "\n",
      "3. Output Statistics:\n",
      "   Mean: -0.0000\n",
      "   Std: 1.0000\n",
      "   Min: -3.7993\n",
      "   Max: 3.5101\n",
      "\n",
      "==================================================\n",
      "✓ ENCODER COMPLETE!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test encoding\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters (same as original Transformer paper)\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    d_ff = 2048  # Typically 4 * d_model\n",
    "    num_layers = 6\n",
    "\n",
    "    # Create input sequence (random embeddings)\n",
    "    x = np.random.randn(seq_len, d_model)\n",
    "\n",
    "    # Add positional encoding\n",
    "    pe = positional_encoding(seq_len, d_model)\n",
    "    x_with_pos = x + pe\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TRANSFORMER ENCODER TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Test single encoder layer\n",
    "    print(\"\\n1. Single Encoder Layer:\")\n",
    "    layer_output = encoder_layer(x_with_pos, d_model, num_heads, d_ff)\n",
    "    print(f\"   Input shape: {x_with_pos.shape}\")\n",
    "    print(f\"   Output shape: {layer_output.shape}\")\n",
    "    print(f\"   ✓ Shapes match!\")\n",
    "\n",
    "    # Test full encoder (6 layers stacked)\n",
    "    print(\"\\n2. Full Encoder (6 layers):\")\n",
    "    encoder_output = transformer_encoder(\n",
    "        x_with_pos, num_layers, d_model, num_heads, d_ff\n",
    "    )\n",
    "    print(f\"   Input shape: {x_with_pos.shape}\")\n",
    "    print(f\"   Output shape: {encoder_output.shape}\")\n",
    "    print(f\"   ✓ Shapes match!\")\n",
    "\n",
    "    # Verify output statistics\n",
    "    print(\"\\n3. Output Statistics:\")\n",
    "    print(f\"   Mean: {encoder_output.mean():.4f}\")\n",
    "    print(f\"   Std: {encoder_output.std():.4f}\")\n",
    "    print(f\"   Min: {encoder_output.min():.4f}\")\n",
    "    print(f\"   Max: {encoder_output.max():.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"✓ ENCODER COMPLETE!\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create mask to prevent attending to future positions\n",
    "\n",
    "    Args:\n",
    "        seq_len: sequence length\n",
    "\n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len) with -inf in upper triangle\n",
    "    \"\"\"\n",
    "    # Create upper triangular matrix of -inf\n",
    "    mask = np.triu(np.ones((seq_len, seq_len)) * -1e9, k=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97d3621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention(decoder_input, encoder_output, d_model, num_heads):\n",
    "    \"\"\"\n",
    "    Cross-attention: decoder attends to encoder output\n",
    "\n",
    "    Args:\n",
    "        decoder_input: queries from decoder (seq_len_dec, d_model)\n",
    "        encoder_output: keys/values from encoder (seq_len_enc, d_model)\n",
    "        d_model: model dimension\n",
    "        num_heads: number of attention heads\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len_dec, d_model)\n",
    "        attention_weights: list of attention matrices\n",
    "    \"\"\"\n",
    "    seq_len_dec, _ = decoder_input.shape\n",
    "    seq_len_enc, _ = encoder_output.shape\n",
    "\n",
    "    assert d_model % num_heads == 0\n",
    "    d_k = d_model // num_heads\n",
    "\n",
    "    # Weight matrices\n",
    "    W_q = np.random.randn(d_model, d_model) * 0.01  # Queries from decoder\n",
    "    W_k = np.random.randn(d_model, d_model) * 0.01  # Keys from encoder\n",
    "    W_v = np.random.randn(d_model, d_model) * 0.01  # Values from encoder\n",
    "    W_o = np.random.randn(d_model, d_model) * 0.01\n",
    "\n",
    "    # Q from decoder, K and V from encoder\n",
    "    Q = decoder_input @ W_q\n",
    "    K = encoder_output @ W_k\n",
    "    V = encoder_output @ W_v\n",
    "\n",
    "    # Reshape for multi-head\n",
    "    Q = Q.reshape(seq_len_dec, num_heads, d_k).transpose(1, 0, 2)\n",
    "    K = K.reshape(seq_len_enc, num_heads, d_k).transpose(1, 0, 2)\n",
    "    V = V.reshape(seq_len_enc, num_heads, d_k).transpose(1, 0, 2)\n",
    "\n",
    "    # Apply attention to each head\n",
    "    head_outputs = []\n",
    "    all_attention_weights = []\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        head_output, attn_weights = scaled_dot_product_attention(Q[i], K[i], V[i])\n",
    "        head_outputs.append(head_output)\n",
    "        all_attention_weights.append(attn_weights)\n",
    "\n",
    "    # Concatenate heads\n",
    "    concat_heads = np.stack(head_outputs, axis=1)\n",
    "    concat_heads = concat_heads.reshape(seq_len_dec, d_model)\n",
    "\n",
    "    # Output projection\n",
    "    output = concat_heads @ W_o\n",
    "\n",
    "    return output, all_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(x, encoder_output, d_model, num_heads, d_ff):\n",
    "    \"\"\"\n",
    "    Single Transformer decoder layer\n",
    "\n",
    "    Args:\n",
    "        x: decoder input (seq_len, d_model)\n",
    "        encoder_output: output from encoder (seq_len_enc, d_model)\n",
    "        d_model: model dimension\n",
    "        num_heads: number of attention heads\n",
    "        d_ff: feed-forward hidden dimension\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    seq_len = x.shape[0]\n",
    "\n",
    "    # 1. Masked self-attention (decoder can't look ahead)\n",
    "    mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "    # Use multi_head_attention but with mask\n",
    "    # We need to modify multi_head_attention to accept mask\n",
    "    # For now, we'll do it manually here\n",
    "    W_q = np.random.randn(d_model, d_model) * 0.01\n",
    "    W_k = np.random.randn(d_model, d_model) * 0.01\n",
    "    W_v = np.random.randn(d_model, d_model) * 0.01\n",
    "    W_o = np.random.randn(d_model, d_model) * 0.01\n",
    "\n",
    "    d_k = d_model // num_heads\n",
    "\n",
    "    Q = (x @ W_q).reshape(seq_len, num_heads, d_k).transpose(1, 0, 2)\n",
    "    K = (x @ W_k).reshape(seq_len, num_heads, d_k).transpose(1, 0, 2)\n",
    "    V = (x @ W_v).reshape(seq_len, num_heads, d_k).transpose(1, 0, 2)\n",
    "\n",
    "    head_outputs = []\n",
    "    for i in range(num_heads):\n",
    "        head_output, _ = scaled_dot_product_attention(Q[i], K[i], V[i], mask=mask)\n",
    "        head_outputs.append(head_output)\n",
    "\n",
    "    masked_attn_output = np.stack(head_outputs, axis=1).reshape(seq_len, d_model) @ W_o\n",
    "\n",
    "    # Add & Norm\n",
    "    x = layer_norm(x + masked_attn_output)\n",
    "\n",
    "    # 2. Cross-attention to encoder output\n",
    "    cross_attn_output, _ = cross_attention(x, encoder_output, d_model, num_heads)\n",
    "\n",
    "    # Add & Norm\n",
    "    x = layer_norm(x + cross_attn_output)\n",
    "\n",
    "    # 3. Feed-forward network\n",
    "    ff_output = feed_forward(x, d_model, d_ff)\n",
    "\n",
    "    # Add & Norm\n",
    "    output = layer_norm(x + ff_output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_decoder(x, encoder_output, num_layers, d_model, num_heads, d_ff):\n",
    "    \"\"\"\n",
    "    Stack of decoder layers\n",
    "\n",
    "    Args:\n",
    "        x: decoder input with positional encoding (seq_len, d_model)\n",
    "        encoder_output: output from encoder (seq_len_enc, d_model)\n",
    "        num_layers: number of decoder layers\n",
    "        d_model: model dimension\n",
    "        num_heads: number of attention heads\n",
    "        d_ff: feed-forward hidden dimension\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    for i in range(num_layers):\n",
    "        x = decoder_layer(x, encoder_output, d_model, num_heads, d_ff)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc45ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRANSFORMER DECODER TEST\n",
      "==================================================\n",
      "\n",
      "1. Running Encoder:\n",
      "   Encoder output shape: (10, 512)\n",
      "\n",
      "2. Running Decoder:\n",
      "\n",
      "3. Single Decoder Layer:\n",
      "   Decoder input shape: (8, 512)\n",
      "   Encoder output shape: (10, 512)\n",
      "   Decoder layer output shape: (8, 512)\n",
      "\n",
      "4. Full Decoder (6 layers):\n",
      "   Decoder output shape: (8, 512)\n",
      "\n",
      "5. Look-Ahead Mask (prevents future attention):\n",
      "   First 5x5 positions:\n",
      "[[ 0.e+00 -1.e+09 -1.e+09 -1.e+09 -1.e+09]\n",
      " [ 0.e+00  0.e+00 -1.e+09 -1.e+09 -1.e+09]\n",
      " [ 0.e+00  0.e+00  0.e+00 -1.e+09 -1.e+09]\n",
      " [ 0.e+00  0.e+00  0.e+00  0.e+00 -1.e+09]\n",
      " [ 0.e+00  0.e+00  0.e+00  0.e+00  0.e+00]]\n",
      "   (0 = can attend, -inf = masked)\n",
      "\n",
      "6. Output Statistics:\n",
      "   Mean: 0.0000\n",
      "   Std: 1.0000\n",
      "   Min: -3.5534\n",
      "   Max: 3.0543\n",
      "\n",
      "==================================================\n",
      "✓ DECODER COMPLETE!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    seq_len_enc = 10  # Encoder sequence length\n",
    "    seq_len_dec = 8  # Decoder sequence length (can be different)\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    num_layers = 6\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TRANSFORMER DECODER TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create encoder input and get encoder output\n",
    "    print(\"\\n1. Running Encoder:\")\n",
    "    encoder_input = np.random.randn(seq_len_enc, d_model)\n",
    "    pe_enc = positional_encoding(seq_len_enc, d_model)\n",
    "    encoder_input_with_pos = encoder_input + pe_enc\n",
    "    encoder_output = transformer_encoder(\n",
    "        encoder_input_with_pos, num_layers, d_model, num_heads, d_ff\n",
    "    )\n",
    "    print(f\"   Encoder output shape: {encoder_output.shape}\")\n",
    "\n",
    "    # Create decoder input\n",
    "    print(\"\\n2. Running Decoder:\")\n",
    "    decoder_input = np.random.randn(seq_len_dec, d_model)\n",
    "    pe_dec = positional_encoding(seq_len_dec, d_model)\n",
    "    decoder_input_with_pos = decoder_input + pe_dec\n",
    "\n",
    "    # Test single decoder layer\n",
    "    print(\"\\n3. Single Decoder Layer:\")\n",
    "    layer_output = decoder_layer(\n",
    "        decoder_input_with_pos, encoder_output, d_model, num_heads, d_ff\n",
    "    )\n",
    "    print(f\"   Decoder input shape: {decoder_input_with_pos.shape}\")\n",
    "    print(f\"   Encoder output shape: {encoder_output.shape}\")\n",
    "    print(f\"   Decoder layer output shape: {layer_output.shape}\")\n",
    "\n",
    "    # Test full decoder\n",
    "    print(\"\\n4. Full Decoder (6 layers):\")\n",
    "    decoder_output = transformer_decoder(\n",
    "        decoder_input_with_pos, encoder_output, num_layers, d_model, num_heads, d_ff\n",
    "    )\n",
    "    print(f\"   Decoder output shape: {decoder_output.shape}\")\n",
    "\n",
    "    # Test look-ahead mask\n",
    "    print(\"\\n5. Look-Ahead Mask (prevents future attention):\")\n",
    "    mask = create_look_ahead_mask(5)\n",
    "    print(\"   First 5x5 positions:\")\n",
    "    print(mask)\n",
    "    print(\"   (0 = can attend, -inf = masked)\")\n",
    "\n",
    "    print(\"\\n6. Output Statistics:\")\n",
    "    print(f\"   Mean: {decoder_output.mean():.4f}\")\n",
    "    print(f\"   Std: {decoder_output.std():.4f}\")\n",
    "    print(f\"   Min: {decoder_output.min():.4f}\")\n",
    "    print(f\"   Max: {decoder_output.max():.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"✓ DECODER COMPLETE!\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bdd56c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformervenv (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
